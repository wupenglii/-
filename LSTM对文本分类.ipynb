{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "## 设置字体\n",
    "from matplotlib.font_manager import FontProperties\n",
    "fonts = FontProperties(fname = \"/Library/Fonts/华文细黑.ttf\",size=14)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('F:/DataSet/cnews/cnews.train.txt',names=['label','text'],encoding='utf-8')\n",
    "train_df = train_df.dropna()\n",
    "test_df = pd.read_table('F:/DataSet/cnews/cnews.test.txt',names=['label','text'],encoding='utf-8')\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道  谢亚龙已经被公安...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0    体育  马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...\n",
       "1    体育  商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...\n",
       "2    体育  冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...\n",
       "3    体育  辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...\n",
       "4    体育  揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道  谢亚龙已经被公安..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train = train_df.text.values.tolist()\n",
    "content_test = test_df.text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.228 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "content_S_train = []\n",
    "for line in content_train:\n",
    "    current_segment = jieba.lcut(line)\n",
    "    if len(current_segment) > 1 and current_segment != '\\r\\n':\n",
    "        content_S_train.append(current_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_S_test = []\n",
    "for line in content_test:\n",
    "    current_segment = jieba.lcut(line)\n",
    "    if len(current_segment) > 1 and current_segment != '\\r\\n':\n",
    "        content_S_test.append(current_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cutword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[马晓旭, 意外, 受伤, 让, 国奥, 警惕,  , 无奈, 大雨, 格外, 青睐, 殷家...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[商瑞华, 首战, 复仇, 心切,  , 中国, 玫瑰, 要, 用, 美国, 方式, 攻克,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[冠军, 球队, 迎新, 欢乐, 派对,  , 黄旭获, 大奖, 张军, 赢, 下, PK,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[辽足, 签约, 危机, 引, 注册, 难关,  , 高层, 威逼利诱, 合同, 笑里藏刀,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[揭秘, 谢亚龙, 被, 带走, ：, 总局, 电话, 骗局,  , 复制, 南杨, 轨迹,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cutword\n",
       "0  [马晓旭, 意外, 受伤, 让, 国奥, 警惕,  , 无奈, 大雨, 格外, 青睐, 殷家...\n",
       "1  [商瑞华, 首战, 复仇, 心切,  , 中国, 玫瑰, 要, 用, 美国, 方式, 攻克,...\n",
       "2  [冠军, 球队, 迎新, 欢乐, 派对,  , 黄旭获, 大奖, 张军, 赢, 下, PK,...\n",
       "3  [辽足, 签约, 危机, 引, 注册, 难关,  , 高层, 威逼利诱, 合同, 笑里藏刀,...\n",
       "4  [揭秘, 谢亚龙, 被, 带走, ：, 总局, 电话, 骗局,  , 复制, 南杨, 轨迹,..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_train=pd.DataFrame({'cutword':content_S_train})\n",
    "df_content_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cutword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[鲍勃, 库西, 奖归, 谁, 属, ？,  , NCAA, 最强, 控卫, 是, 坎巴, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[麦基, 砍, 28, +, 18, +, 5, 却, 充满, 寂寞,  , 纪录, 之夜,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[黄蜂, vs, 湖人, 首发, ：, 科比, 冲击, 七, 连胜,  , 火箭, 两旧, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[双面, 谢亚龙, 作秀, 终成, 做作,  , 谁, 来, 为, 低劣, 行政, 能力, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[兔年, 首战, 山西, 换帅, 后, 有, 虎胆,  , 张学文, 用, 乔丹, 名言, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cutword\n",
       "0  [鲍勃, 库西, 奖归, 谁, 属, ？,  , NCAA, 最强, 控卫, 是, 坎巴, ...\n",
       "1  [麦基, 砍, 28, +, 18, +, 5, 却, 充满, 寂寞,  , 纪录, 之夜,...\n",
       "2  [黄蜂, vs, 湖人, 首发, ：, 科比, 冲击, 七, 连胜,  , 火箭, 两旧, ...\n",
       "3  [双面, 谢亚龙, 作秀, 终成, 做作,  , 谁, 来, 为, 低劣, 行政, 能力, ...\n",
       "4  [兔年, 首战, 山西, 换帅, 后, 有, 虎胆,  , 张学文, 用, 乔丹, 名言, ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_test=pd.DataFrame({'cutword':content_S_test})\n",
    "df_content_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"F:/PythonClassCode/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'],encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords(contents,stopwords):\n",
    "    contents_clean = []\n",
    "    all_words = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "            all_words.append(str(word))\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean,all_words\n",
    "\n",
    "contents_train = df_content_train.cutword.values.tolist()\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "contents_clean_train,all_words_train = drop_stopwords(contents_train,stopwords)\n",
    "\n",
    "contents_test = df_content_test.cutword.values.tolist()\n",
    "contents_clean_test,all_words_test = drop_stopwords(contents_test,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content_train = pd.DataFrame({'contents_clean_train':contents_clean_train})\n",
    "df_content_test = pd.DataFrame({'contents_clean_test':contents_clean_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words_train = pd.DataFrame({'all_words_train':all_words_train})\n",
    "df_all_words_test = pd.DataFrame({'all_words_test':all_words_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'label':train_df['label'],'cutword':contents_clean_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'label':test_df['label'],'cutword':contents_clean_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cutword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>[马晓旭, 意外, 受伤, 国奥, 警惕,  , 无奈, 大雨, 青睐, 殷家, 军, 傅亚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>[商瑞华, 首战, 复仇, 心切,  , 中国, 玫瑰, 美国, 方式, 攻克, 瑞典, 多...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>[冠军, 球队, 迎新, 欢乐, 派对,  , 黄旭获, 大奖, 张军, 赢, PK, 赛,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>[辽足, 签约, 危机, 引, 注册, 难关,  , 高层, 威逼利诱, 合同, 笑里藏刀,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>[揭秘, 谢亚龙, 带走, 总局, 电话, 骗局,  , 复制, 南杨, 轨迹, 体坛周报,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            cutword\n",
       "0    体育  [马晓旭, 意外, 受伤, 国奥, 警惕,  , 无奈, 大雨, 青睐, 殷家, 军, 傅亚...\n",
       "1    体育  [商瑞华, 首战, 复仇, 心切,  , 中国, 玫瑰, 美国, 方式, 攻克, 瑞典, 多...\n",
       "2    体育  [冠军, 球队, 迎新, 欢乐, 派对,  , 黄旭获, 大奖, 张军, 赢, PK, 赛,...\n",
       "3    体育  [辽足, 签约, 危机, 引, 注册, 难关,  , 高层, 威逼利诱, 合同, 笑里藏刀,...\n",
       "4    体育  [揭秘, 谢亚龙, 带走, 总局, 电话, 骗局,  , 复制, 南杨, 轨迹, 体坛周报,..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'F:\\DataSet\\cnews\\cnews_train_cut.csv')\n",
    "test_df = pd.read_csv(r'F:\\DataSet\\cnews\\cnews_test_cut.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>classification</th>\n",
       "      <th>content</th>\n",
       "      <th>content_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>体育</td>\n",
       "      <td>马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...</td>\n",
       "      <td>['马晓旭', '意外', '受伤', '让', '国奥', '警惕', '大雨', '格外...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>体育</td>\n",
       "      <td>商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...</td>\n",
       "      <td>['商瑞华', '首战', '复仇', '心切', '玫瑰', '要', '用', '美国'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>体育</td>\n",
       "      <td>冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...</td>\n",
       "      <td>['冠军', '球队', '迎新', '欢乐', '派对', '获', '大奖', '张军'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>体育</td>\n",
       "      <td>辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...</td>\n",
       "      <td>['辽', '足', '签约', '危机', '引', '注册', '难关', '威逼利诱'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>体育</td>\n",
       "      <td>揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道? 谢亚龙已经被公安...</td>\n",
       "      <td>['揭秘', '谢亚龙', '被', '带走', '电话', '骗局', '南杨', '轨迹...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 classification  \\\n",
       "0           0             体育   \n",
       "1           1             体育   \n",
       "2           2             体育   \n",
       "3           3             体育   \n",
       "4           4             体育   \n",
       "\n",
       "                                             content  \\\n",
       "0  马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...   \n",
       "1  商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...   \n",
       "2  冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...   \n",
       "3  辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...   \n",
       "4  揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道? 谢亚龙已经被公安...   \n",
       "\n",
       "                                       content_short  \n",
       "0  ['马晓旭', '意外', '受伤', '让', '国奥', '警惕', '大雨', '格外...  \n",
       "1  ['商瑞华', '首战', '复仇', '心切', '玫瑰', '要', '用', '美国'...  \n",
       "2  ['冠军', '球队', '迎新', '欢乐', '派对', '获', '大奖', '张军'...  \n",
       "3  ['辽', '足', '签约', '危机', '引', '注册', '难关', '威逼利诱'...  \n",
       "4  ['揭秘', '谢亚龙', '被', '带走', '电话', '骗局', '南杨', '轨迹...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cutword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>['马晓旭', '意外', '受伤', '让', '国奥', '警惕', '大雨', '格外...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>['商瑞华', '首战', '复仇', '心切', '玫瑰', '要', '用', '美国'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>['冠军', '球队', '迎新', '欢乐', '派对', '获', '大奖', '张军'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>['辽', '足', '签约', '危机', '引', '注册', '难关', '威逼利诱'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>['揭秘', '谢亚龙', '被', '带走', '电话', '骗局', '南杨', '轨迹...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            cutword\n",
       "0    体育  ['马晓旭', '意外', '受伤', '让', '国奥', '警惕', '大雨', '格外...\n",
       "1    体育  ['商瑞华', '首战', '复仇', '心切', '玫瑰', '要', '用', '美国'...\n",
       "2    体育  ['冠军', '球队', '迎新', '欢乐', '派对', '获', '大奖', '张军'...\n",
       "3    体育  ['辽', '足', '签约', '危机', '引', '注册', '难关', '威逼利诱'...\n",
       "4    体育  ['揭秘', '谢亚龙', '被', '带走', '电话', '骗局', '南杨', '轨迹..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame({'label':train_df['classification'],'cutword':train_df['content_short']})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cutword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>['鲍勃', '库西', '奖', '归', '谁', '属', 'NCAA', '最强',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>['麦基', '砍', '28', '却', '充满', '寂寞', '之', '夜', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>['黄蜂', 'vs', '湖人', '首发', '冲击', '七', '连胜', '两',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>['双面', '谢亚龙', '作秀', '终成', '做作', '来', '为', '低劣'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>['兔年', '首战', '山西', '换帅', '后', '有', '虎胆', '用', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            cutword\n",
       "0    体育  ['鲍勃', '库西', '奖', '归', '谁', '属', 'NCAA', '最强',...\n",
       "1    体育  ['麦基', '砍', '28', '却', '充满', '寂寞', '之', '夜', '...\n",
       "2    体育  ['黄蜂', 'vs', '湖人', '首发', '冲击', '七', '连胜', '两',...\n",
       "3    体育  ['双面', '谢亚龙', '作秀', '终成', '做作', '来', '为', '低劣'...\n",
       "4    体育  ['兔年', '首战', '山西', '换帅', '后', '有', '虎胆', '用', ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({'label':test_df['classification'],'cutword':test_df['content_short']})\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df.label\n",
    "test_y = test_df.label\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y).reshape(-1,1)\n",
    "test_y = le.transform(test_y).reshape(-1,1)\n",
    "\n",
    "## 对数据集的标签数据进行one-hot编码\n",
    "ohe = OneHotEncoder()\n",
    "train_y = ohe.fit_transform(train_y).toarray()\n",
    "test_y = ohe.transform(test_y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'的'\", 1)\n",
      "(\"'在'\", 2)\n",
      "(\"'了'\", 3)\n",
      "(\"'是'\", 4)\n",
      "(\"'月'\", 5)\n",
      "(\"'和'\", 6)\n",
      "(\"'日'\", 7)\n",
      "(\"'中'\", 8)\n",
      "(\"'有'\", 9)\n",
      "(\"'将'\", 10)\n",
      "===================\n",
      "(\"'马晓旭'\", 1)\n",
      "(\"'意外'\", 414)\n",
      "(\"'受伤'\", 600)\n",
      "(\"'让'\", 7314)\n",
      "(\"'国奥'\", 32)\n",
      "(\"'警惕'\", 76)\n",
      "(\"'大雨'\", 19)\n",
      "(\"'格外'\", 197)\n",
      "(\"'青睐'\", 314)\n",
      "(\"'殷家'\", 1)\n"
     ]
    }
   ],
   "source": [
    "max_words = 5000\n",
    "max_len = 600\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(train_df.cutword)\n",
    "\n",
    "for ii,iterm in enumerate(tok.word_index.items()):\n",
    "    if ii < 10:\n",
    "        print(iterm)\n",
    "    else:\n",
    "        break\n",
    "print(\"===================\")  \n",
    "for ii,iterm in enumerate(tok.word_counts.items()):\n",
    "    if ii < 10:\n",
    "        print(iterm)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 600)\n",
      "(10000, 600)\n"
     ]
    }
   ],
   "source": [
    "train_seq = tok.texts_to_sequences(train_df.cutword)\n",
    "test_seq = tok.texts_to_sequences(test_df.cutword)\n",
    "## 将每个序列调整为相同的长度\n",
    "train_seq_mat = sequence.pad_sequences(train_seq,maxlen=max_len)\n",
    "test_seq_mat = sequence.pad_sequences(test_seq,maxlen=max_len)\n",
    "\n",
    "print(train_seq_mat.shape)\n",
    "print(test_seq_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 600, 128)          640128    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 789,514\n",
      "Trainable params: 789,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(name='inputs',shape=[max_len])\n",
    "## Embedding(词汇表大小,batch大小,每个新闻的词长)\n",
    "layer = Embedding(max_words+1,128,input_length=max_len)(inputs)\n",
    "layer = LSTM(128)(layer)\n",
    "layer = Dense(128,activation=\"relu\",name=\"FC1\")(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(10,activation=\"softmax\",name=\"FC2\")(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=RMSprop(),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10368/50000 [=====>........................] - ETA: 27:36 - loss: 1.7020 - acc: 0.3165"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-50329a78798a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model_fit = model.fit(train_seq_mat,train_y,batch_size=128,epochs=10,\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                       \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m## 当val-loss不再提升时停止训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                      )\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(train_seq_mat,train_y,batch_size=128,epochs=10,                      \n",
    "                      callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)] \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d5bee87fc4f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 对测试集进行预测\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seq_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m## 评价预测效果，计算混淆矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconfm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "## 对测试集进行预测\n",
    "test_pre = model.predict(test_seq_mat)\n",
    "\n",
    "## 评价预测效果，计算混淆矩阵\n",
    "confm = metrics.confusion_matrix(np.argmax(test_pre,axis=1),np.argmax(test_y,axis=1))\n",
    "## 混淆矩阵可视化\n",
    "Labname = [\"体育\",\"娱乐\",\"家居\",\"房产\",\"教育\",\"时尚\",\"时政\",\"游戏\",\"科技\",\"财经\"]\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(confm.T, square=True, annot=True,\n",
    "            fmt='d', cbar=False,linewidths=.8,\n",
    "            cmap=\"YlGnBu\")\n",
    "plt.xlabel('True label',size = 14)\n",
    "plt.ylabel('Predicted label',size = 14)\n",
    "plt.xticks(np.arange(10)+0.5,Labname,fontproperties = fonts,size = 12)\n",
    "plt.yticks(np.arange(10)+0.3,Labname,fontproperties = fonts,size = 12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(metrics.classification_report(np.argmax(test_pre,axis=1),np.argmax(test_y,axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
